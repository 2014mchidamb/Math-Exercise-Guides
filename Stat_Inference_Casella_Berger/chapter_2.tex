\section{Transformations and Expectations}

\subsection{Exercise 1}
(a) $F_Y(y) = P(X^3 \leq y) = F_X (y^{\frac{1}{3}})$. Differentiating, we get that 
$f_Y(y) = 14y(1 - y^{\frac{1}{3}})$ which integrates to 1 over $(0, 1)$.

(b)  $f_Y(y) = \dv{y} F_X(\frac{y - 3}{4}) = \frac{7}{4} \exp(\frac{-7(y - 3)}{4})$, which again
integrates to 1 over $(3, \infty)$. The sample space of $Y$ consists of $(3, \infty)$ because 
$4X + 3$ is monotonically increasing and $4(0) + 3 = 3$.

(c) We need only consider positive square roots, since  $x \in (0, 1)$. Thus,
$f_Y(y) = \frac{1}{2\sqrt{y}}30y (1 - \sqrt{y})^2$, which integrates to 1 over $(0, 1)$.

\subsection{Exercise 2}
(a) $f_Y(y) = \frac{1}{2\sqrt{y}} f_X(\sqrt{y}) = \frac{1}{2\sqrt{y}}$

(b) Plug $x = e^{-y}$ into $f_X(x)$ and multiply by $e^{-y}$ (the negative of the derivative, since $-\log X$
is decreasing).

(c) Plug in  $x = \log y$ and multiply by $\frac{1}{y}$.

\subsection{Exercise 3}
From the definition of $Y$, we can see that the sample space is
$\mathcal{Y} = \big(\frac{n}{n+1}\big)_{\mathbb{N}}$. Solving $y = \frac{x}{x + 1}$ for $y$, we find that
$f_Y(y) = f_X(\frac{y}{1 - y})$.

\subsection{Exercise 4}
(a) Integrating from $-\infty$ to 0 gives $\frac{1}{2}$ and likewise for 0 to  $\infty$, so $f$ is a pdf.

(b) If $t \leq 0$, then we have that $P(X < t) = \frac{1}{2} e^{\lambda t}$. Otherwise, 
\begin{align*}
        P(X < t) &= \int_{-\infty}^{t} f(x) = \int_{-\infty}^{0} f(x) + \int_{0}^{t} f(x) \\ 
                 &= \frac{1}{2} + \frac{1}{2} - \frac{1}{2} e^{-\lambda t} \\
                 &= 1 - \frac{1}{2}e^{-\lambda t}
\end{align*}

(c) If $t \leq 0$, $P(\abs{X} < t)$ is clearly 0. Otherwise,
\begin{align*}
        P(\abs{X} < t) &= P(-t < X < t) = P(X < t) - P(X < -t) \\
                       &= 1 - e^{- \lambda t} 
\end{align*}
from the CDFs computed in part (b). 

\subsection{Exercise 5}
The sample space $\mathcal{Y}$ is $[0, 1]$. Thus, we need to consider 
$P(Y \leq y) = P(X \leq \sin^{-1} \sqrt{y})$ for $y \in [0, 1]$. By symmetry, we can just consider the case
where $\sin^{-1}$ is restricted to $[0, \frac{\pi}{2}]$, as the other three quadrants have the same area.
Therefore,
\begin{align*}
        f_Y(y) &= 4 \dv{y} F_X(\sin^{-1} \sqrt{y}) \\
               &= 4 \dv{y} \frac{\sin^{-1} \sqrt{y}}{2\pi} \\
               &= \frac{1}{\pi \sqrt{y (1 - y)}}
\end{align*}
At $y = 0$ and $y = 1$, the density is infinite/undefined - I'm not really sure how to interpret this.

\subsection{Exercise 11}
(a) We have that 
\begin{align*}
        \mathrm{E} [X^2] = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty} x^2 e^{-\frac{x^2}{2}} &= \frac{2}{\sqrt{2\pi}} \int_{0}^{\infty} x^2 e^{-\frac{x^2}{2}} \\
                                                                            &= \frac{2}{\sqrt{2\pi}} \big(\lim_{x \to \infty} -x e^{-\frac{x^2}{2}} + \int_{0}^{\infty} e^{-\frac{x^2}{2}}\big) \\
                                                                            &= 1
\end{align*}
Where we integrated by parts using $dv = xe^{-\frac{x^2}{2}}$ and $u = x$, applied L'H\^opital's rule, 
and then compared to the CDF of the normal distribution.

(b) The pdf $f_Y(y)$ is just $f_X(y) + f_X(-y)$, so $f_Y(y) = \frac{2}{\sqrt{2\pi}} e^{-\frac{y^2}{2}}$.
We can then compute $\mathrm{E} [Y]$ as
\begin{align*}
        \mathrm{E} [Y] = \frac{2}{\sqrt{2\pi}} \int_{0}^{\infty} y e^{-\frac{y^2}{2}} &= \frac{2}{\sqrt{2\pi}}  
\end{align*}
To compute the variance we need to compute $\mathrm{E} [Y^2]$, which is identical to $\mathrm{E} [X^2]$. Thus, 
$\mathrm{Var} [Y] = \mathrm{E} [Y^2] - \mathrm{E} [Y]^2 = 1 - \frac{2}{\pi}$.

\subsection{Exercise 12}
We see that $y = d \tan(x)$. Since $\tan(x)$ is increasing on $(0, \frac{\pi}{2})$, we get that
\begin{align*}
        f_Y(y) &= F_X\big(\tan^{-1} \frac{y}{d}\big) \dv{y} \tan^{-1} \frac{y}{d} \\
               &= \frac{2\tan^{-1} \frac{y}{d}}{\pi d\big(1 + \frac{y^2}{d^2}\big)}
\end{align*}
for $y \in (0, \infty)$. We can compute $\mathrm{E} [Y]$ directly from $f_X(x)$ as
\begin{align*}
        \mathrm{E} [Y] &= \frac{2d}{\pi}\int_{0}^{\frac{\pi }{2}} \tan(x) dx \\
                       &= \infty
\end{align*}
So $\mathrm{E} [Y]$ does not exist.

\subsection{Exercise 13}
A sequence of flips has length $l$ if there are either $l$ heads in a row or $l$ tails in a row, so
$P(X = l) = p^l (1 - p) + (1 - p)^l p = p \text{Geom}(1 - p) + (1 - p) \text{Geom}(p)$. Therefore,
by linearity of expected value, $\mathrm{E} [X] = \frac{p}{1 - p} + \frac{1 - p}{p}$.

\subsection{Exercise 14}
(a) We have that
\begin{align*}
        \int_{0}^{\infty} 1 - F_X(x) dx = \int_{x = 0}^{\infty} \int_{y = x}^{\infty} f_X(y) dy dx  
\end{align*}
Since $0 \leq x \leq \infty$ and $x \leq y \leq \infty$, we can change the order of integration by having
$0 \leq y \leq \infty$ on the outside and $0 \leq x \leq y$ on the inside
\begin{align*}
        \int_{x = 0}^{\infty} \int_{y = x}^{\infty} f_X(y) dy dx &= \int_{y = 0}^{\infty} \int_{x = 0}^{y} dx f_X(y) dy \\
                                                                 &= \int_{0}^{\infty} y f_X(y) dy \\
                                                                 &= E[X]
\end{align*}


(b) We can rewrite the expected value as a sum of infinite sums to see that
\begin{align*}
        E[X] &= \sum_{k = 1}^\infty k f(k) = \sum_{k = 1}^\infty f(k) + \sum_{k = 2}^\infty f(k) + ... \\
             &= \sum_{k = 0}^\infty \sum_{j = k + 1}^\infty f(k) = \sum_{k = 0}^\infty (1 - F_X(k))
\end{align*}

\subsection{Exercise 17}
(a) We need to solve $m^3 = 1 - m^3$, since $F_X(x) = x^3$. Solving gives $m = \sqrt[3]{\frac{1}{2}}$.

(b) $f$ is an even function, so $m = 0$.

\subsection{Exercise 18}
We differentiate under the integral sign to get
\begin{align*}
        \dv{a} \mathrm{E} [\abs{X - a}] &= \dv{a} \bigg(\int_{a}^{\infty} (x - a) f_X(x) dx + \int_{-\infty}^{a} (a - x) f_X(x) dx\bigg) \\
                                        &= \int_{a}^{\infty} -f_X(x) dx + \int_{-\infty}^{a} f_X(x) dx \\  
                                        &= 2F_X(a) - 1
\end{align*}
Setting the last line equal to 0 yields $a = m$. To see that this is a minimum, we note that $2F_X(a) - 1 \leq 0$
when $a < m$ and $2F_X(a) - 1 \geq 0$ when $a > m$.

\subsection{Exercise 25}
(a)
\begin{align*}
        F_{-X}(x) = P(X \geq -x) &= 1 - F_X(-x) \\
                                 &= \int_{-x}^{\infty} f_X(x) dx \\
                                 &= \int_{-x}^{0} f_X(x) dx + \int_{0}^{\infty} f_X(x) dx \\
                                 &= \int_{0}^{x} f_X(x) dx + \int_{-\infty}^{0} f_X(x) dx \\  
                                 &= F_X(x)
\end{align*}

(b)
\begin{align*}
        M_X(t) &= \int_{-\infty}^{\infty} e^{tx} f_X(x) dx \\ 
               &= \int_{-\infty}^{0} e^{tx} f_X(x) dx + \int_{0}^{\infty} e^{tx} f_X(x) dx \\  
               &= \int_{0}^{\infty} e^{-tx} f_X(x) dx + \int_{-\infty}^{0} e^{-tx} f_X(x) dx \\
               &= M_X(-t)
\end{align*}

\subsection{Exercise 29}
(a) For binomial, we have
\begin{align*}
        \mathrm{E} [X(X - 1)] &= \sum_{k = 0}^n k(k - 1) \binom{n}{k} p^k (1 - p)^{n - k} \\
                              &= n (n - 1) p^2 \sum_{k = 2}^n \binom{n - 2}{k - 2} p^(k - 2) (1 - p)^{(n - 2) - (k - 2)} \\
                              &= n (n - 1) p^2
\end{align*}
For Poisson, we have
\begin{align*}
        \mathrm{E} [X(X - 1)] &= \sum_{k = 0}^{\infty} k(k - 1) \frac{\lambda^k e^{-\lambda}}{k!} \\ 
                              &= \lambda^2 \sum_{k = 2}^{\infty} \frac{\lambda^{k - 2} e^{-\lambda}}{(k - 2)!} \\
                              &= \lambda^2
\end{align*}

(b) To compute the variances, we can use linearity of expectation to see that 
$\mathrm{E} [X(X - 1)] + \mathrm{E} [X] - \mathrm{E} [X]^2 = \mathrm{Var} [X]$. So for binomial, we get
\begin{align*}
        \mathrm{Var} [X] &= n (n - 1) p^2 + np - n^2 p^2 = np - np^2 = np(1 - p)
\end{align*}
And for Poisson, we get
\begin{align*}
        \mathrm{Var} [X] &= \lambda^2 + \lambda - \lambda^2 = \lambda 
\end{align*}

(c) This one is kind of a pain to typeset; the technique is the same as (a), although more involved.

\subsection{Exercise 31}
No such distribution exists, since $M_X(0) = 0$, which contradicts the fact that $\mathrm{E} [1] = 1$.
