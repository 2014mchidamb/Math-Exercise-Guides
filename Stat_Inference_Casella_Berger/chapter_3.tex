\section{Common Families of Distributions}

\subsection{Exercise 1}
\begin{align*}
        \mathrm{E} [X] &= \sum_{i = N_0}^{N_1} \frac{i}{N_1 - N_0 + 1} = \frac{1}{N_1 - N_0 + 1} (\sum_{i = 1}^{N_1} i - \sum_{i = 1}^{N_0 - 1} i) = \frac{N_1 + N_0}{2} \\
        \mathrm{E} [X^2] &= \frac{1}{N_1 - N_0 + 1} (\sum_{i = 1}^{N_1} i^2 - \sum_{i = 1}^{N_0 - 1} i^2) = \frac{2N_0^2 + 2N_0N_1 - N_0 +2N_1^2 + N_1}{6} \\
        \mathrm{Var} [X] &= \mathrm{E} [X^2] - \mathrm{E} [X]^2 = \frac{(N_1 - N_0 + 1)^2 - 1}{12} 
\end{align*}

\subsection{Exercise 2}
(a) Suppose the number of defective parts in the lot is $D$. The likelihood that we accept a sample from such
a lot is $P(X = 0)$, where $X$ is distributed according to $\text{Hypergeom}(100, D, K)$. 
Given that a lot is unacceptable if $D > 5$, we need only consider the case where $D = 6$, since larger $D$ 
lead to lower likelihood of being incorrectly accepted. Thus, we solve
\begin{align*}
        \frac{\binom{94}{K}}{\binom{100}{K}} < 0.1 
\end{align*}
by running the following Python code:
\begin{lstlisting}[language=python]
  from scipy import special

  K = 1
  while special.binom(94, K) / special.binom(100, K) > 0.1:
    K += 1
\end{lstlisting}
to get that $K = 32$.

(b) This is the same as part (a) except that we now have to consider $P(X = 0) + P(X = 1) < 0.1$. Slightly
modifying the Python code above yields $K = 51$.

\subsection{Exercise 3}
At each second, the probability that at least one car passes in the next 3 seconds is $1 - P(X = 0)$, where
$X$ is binomial with $n = 3$ and $p = p$. Thus, the probability that the pedestrian has to wait 4 seconds
before starting to cross is the probability that at least one car passes in the first three seconds multiplied 
by the probabilities that a car crosses at the 4th second and that no cars cross during seconds 5-7. This
probability is $(1 - (1 - p)^3) p (1 - p)^3$.

\textbf{NOTE:} I compared my answer to the actual solutions, but I don't buy that the actual
answer is $(1 - p (1 - p)^3) (1 - p)^3$ based on the logic provided. What if cars pass on the first and second
seconds, but then no cars pass after? The pedestrian could then leave after two seconds.

\subsection{Exercise 4}
(a) If unsuccessful keys are not eliminated, then this is the same as asking for the expectation of a 
geometric random variable with $p = \frac{1}{n}$, so we expect that it will take $n$ trials.

(b) If we eliminate unsuccessful keys, then our expectation looks like  
\begin{align*}
        \mathrm{E} [X] &= \frac{1}{n} + 2 * \frac{n - 1}{n} * \frac{1}{n - 1} + ... \\
                       &= \frac{1}{n} \sum_{i = 1}^n i = \frac{n + 1}{2}
\end{align*}

\subsection{Exercise 5}
The standard drug's effectiveness can be modeled as a binomial distribution with $n = 100$ and $p = 0.8$, since
we are given that the drug is expected to be effective in 80 out of 100 cases. Computing $P(X \geq 85)$ yields
a value of approximately 0.129, which means that it is fairly unlikely for the new drug to work in 85
(or more) cases if it was worse than the first drug. Thus, we conclude that the new drug is superior.

\subsection{Exercise 7}
We would like to have $P(X \geq 2) = 1 - P(X < 2) > 0.99$. Thus, we can solve 
$e^{-\lambda} (1 + \lambda) < 0.01$ to get that $\lambda > 6.638$.

\subsection{Exercise 12}
I tried to do this analytically but struggled to show equivalence between the summations; obtaining the result
through qualitative reasoning is much simpler. $F_X(r - 1)$ is the probability that there are at most $r - 1$
successes in $n$ Bernoulli trials, which is the same as saying that we get success number $r$ after $n$ trials.
Thus, we must have at least $n + 1 - r$ failures, which is $1 - F_Y(n - r)$ (I don't deserve credit for this,
I looked up part of the solution here - maybe I'll retry the analytic approach later).

\subsection{Exercise 15}
I spent an absurd amount of time on this question because I was working off of Wikipedia's provided negative
binomial MGF, which treats $p$ as $1 - p$. After finding the proper MGF, the result is straightforward: 
\begin{align*}
        \lim_{r \to \infty} \bigg(\frac{p}{1 - (1 - p)e^t}\bigg)^r &= 
        \lim_{r \to \infty}\bigg(1 + \frac{p}{1 - (1 - p)e^t} - \frac{1 - (1 - p)e^t}{1 - (1 - p)e^t}\bigg)^r \\
                                                                   &= \lim_{r \to \infty} \bigg(1 + \frac{r\frac{-(1 - p) + (1 - p)e^t}{1 - (1 - p)e^t}}{r}\bigg)^r \\
                                                                   &= \exp(-\lambda + \lambda e^t)
\end{align*}

\subsection{Exercise 19}
Since $\alpha = 1, 2, 3, ...$ we have $\Gamma(\alpha) = (\alpha - 1)!$. We can then integrate by parts with 
$u = \frac{1}{(\alpha - 1)!} z^{\alpha - 1}$ and $dv = e^{-z}$ to get
\begin{align*}
        \int_{x}^{\infty} \frac{1}{(\alpha - 1)!} z^{\alpha - 1} e^{-z} dz &= \frac{x^{\alpha - 1} e^{-x}}{(\alpha - 1)!} + \int_{x}^{\infty} \frac{1}{(\alpha - 2)!} z^{\alpha - 2} e^{-z} dz \\
                                                                           &= \sum_{y = 0}^{\alpha - 1} \frac{x^y e^{-x}}{y!}
\end{align*}
where the last line follows from repeatedly integrating by parts until $z^{\alpha - k} = 0$. Probabilistically,
this equality gives that $P(X \geq x) = P(Y \leq \alpha - 1)$ where $X$ is Gamma distributed with 
 $\alpha = \alpha, \beta = 1$ and $Y$ is Poisson with $\lambda = x$.

\subsection{Exercise 44}
Directly applying Chebychev to $P(\abs{X} \geq b)$ gives $P(\abs{X} \geq b) \leq \frac{\mathrm{E}[\abs{X}]}{b}$.
Furthermore, since $P(\abs{X} \geq b) = P(X^2 \geq b^2)$, we can apply Chebychev again to get
$P(\abs{X} \geq b) \leq \frac{\mathrm{E} [X^2]}{b^2}$. Both $\mathrm{E} [\abs{X}]$ and $\mathrm{E} [X^2]$ 
can be computed via integration by parts as 1 and 2 respectively. Thus, for $b = 3$ the $X^2$ bound is
better and for  $b = \sqrt{2}$ the $\abs{X}$ bound is better.

\subsection{Exercise 45}
(a)
\begin{align*}
        M_X(t) &= \int_{-\infty}^{\infty} e^{tx} f_X(x) dx \\ 
               &\geq e^{at} \int_{x \geq a} f_X(x) dx \quad \text{since } e^{xt} \geq e^{at} \text{ for } x \geq a, t \geq 0 \\
               &\geq e^{at} P(X \geq a)
\end{align*}

(b) Identical to (a), except we use  $x \leq a$ in step 2 since $t < 0$.

(c) We just need $h(t, x)$ to be nonnegative and $h(t, x) \geq 1$ for all $x, t \geq 0$, since then we can
replicate the steps of (a).

