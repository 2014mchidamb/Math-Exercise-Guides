\section{Sequences and Series of Functions}

\subsection{Exercise 1}
We have that $\abs{f_n(x)} \leq M_n$. From uniform convergence, we get that there exists $N$ such that
\begin{align*}
        n, m \geq N \implies \abs{f_n(x) - f_m(x)} < \epsilon \implies \abs{M_n - M_m} < \epsilon
\end{align*}
So $\lim_{n \to \infty} M_n = M$ for some $M$. Thus, we can choose $N$ such that $n \geq N \implies M_n < M + 1$
, so we can set $M_u = \max\{M_1, M_2, ..., M_{N-1}, M + 1\}$. By construction, $M_u$ must be a uniform
bound for all of the $f_n$.

\subsection{Exercise 2}
Since $f_n, g_n$ are both uniformly convergent, we can choose $N_1, N_2$ such that 
\begin{align*}
        n, m \geq \max\{N_1, N_2\} \implies \abs{f_n(x) - f_m(x)} < \frac{\epsilon}{2}, \: \abs{g_n(x) - g_m(x)} < \frac{\epsilon}{2} \\
        \abs{f_n(x) + g_n(x) - f_m(x) - g_m(x)} \leq \abs{f_n(x) - f_m(x)} + \abs{g_n(x) - g_m(x)} < \epsilon
\end{align*}
So $f_n + g_n$ converges uniformly as well. 

Suppose $f_n \to f$ and $g_n \to g$, with $f_n(x) \leq M_f$ and $g_n(x) \leq M_g$. Then $g(x) f_n(x)$ 
converges uniformly since $M_g f_n(x)$ converges uniformly, and likewise for $g_n(x) f(x)$. Thus,
\begin{align*}
        \frac{\epsilon}{3} &> \abs{(f_n(x) - f(x))(g_n(x) - g(x))} \\
                 &> \abs{f_n(x)g_n(x) - f_n(x)g(x) - f(x)g_n(x) + f(x)g(x)} \\
                 &> \abs{f_n(x)g_n(x) - f(x)g(x)} - \abs{f(x)g(x) - f_n(x)g(x)} - \abs{f(x)g(x) - f(x)g_n(x)} \\
                 &> \abs{f_n(x)g_n(x) - f(x)g(x)} - \frac{\epsilon}{3} - \frac{\epsilon}{3} \\
        \implies \epsilon &> \abs{f_n(x)g_n(x) - f(x)g(x)}
\end{align*}
So $f_n g_n \to fg$ uniformly.

\subsection{Exercise 3}
Let $f_n(x) = \frac{1}{x} + \frac{1}{n}$ and $g_n(x) = \frac{1}{x}$ on $E = (0, 1)$. 
Both $f_n$ and $g_n$ converge uniformly, and $f_n(x) g_n(x)$ converges pointwise to $\frac{1}{x^2}$. However,
\begin{align*}
        \abs{f_n(x) g_n(x) - \frac{1}{x^2}} = \abs{\frac{1}{xn}}
\end{align*}
So the convergence of $f_n(x) g_n(x)$ is not uniform on  $E$.

\subsection{Exercise 4}
Since $1 + n^2 x = 0$ whenever $x = -\frac{1}{n^2}$, $f(x)$ is not defined/does not converge for 
$x = 0, -\frac{1}{n^2}$. For all other values of  $x$, however, $f(x)$ converges absolutely since 
$\sum_{n = 1}^\infty \frac{1}{n^2}$ converges. $f(x)$ converges uniformly on any intervals of the form 
$(a, -1)$ and $(b, c)$ with $b > 0$, since such intervals do not contain points of the form $-\frac{1}{n^2}$
and $\frac{1}{x}$ is bounded on all such intervals. $f$ fails to converge uniformly on all other intervals.
$f(x)$ is continuous on the intervals that it converges uniformly on.
$f$ is not bounded since $\frac{1}{x}$ can be made arbitrarily large.

\subsection{Exercise 5}
For every positive real number $x$, there exists $N$ such that $\frac{1}{N} < x$ (Archimedean property). Hence,
choosing $n \geq N \implies f_n(x) = 0$. Since every non-positive real number is less than $\frac{1}{n + 1}$ 
for all $n \in \mathbb{N}$, we have that $f_n$ converges pointwise to the function $f(x) = 0$. 
As such, it is clear that $\sum_{n = 1}^\infty \abs{f_n(x)} = \sum_{n = 1}^N \abs{f_n(x)}$ converges for all $x$
, and yet we do not have uniform convergence.

\subsection{Exercise 6}
We can rewrite the series as
\begin{align*}
        \sum_{n = 1}^\infty (-1)^n \frac{x^2 + n}{n^2} = \sum_{n = 1}^\infty (-1)^n \frac{1}{n} + \sum_{n = 1}^\infty (-1)^n \frac{x^2}{n^2}
\end{align*}
so the series converges uniformly since the alternating harmonic series converges and the series 
$\sum_{n = 1}^\infty \frac{M}{n^2}$ converges (where $x^2 \leq M$, which is possible on every bounded interval).
However, this series also clearly does not converge absolutely for any value of $x$ since the harmonic
series does not converge.

\subsection{Exercise 7}
We posit that $f_n$ converges uniformly to the function $f(x) = 0$. For this to be true, there needs to be a
single $N$ such that $n \geq N$ gives
\begin{align*}
        \abs{\frac{x}{1 + nx^2}} < \epsilon \implies \frac{\abs{x} - \epsilon}{\epsilon x^2} < n
\end{align*}
for all $x$. In other words, the lefthand side of the last expression above must have a maximum. Since the 
lefthand side is differentiable for all $x \neq 0$, we can differentiate and equate to 0 to find that 
\begin{align*}
        \dv{x} \frac{x - \epsilon}{\epsilon x^2} = \frac{2\epsilon^2 x - \epsilon x^2}{(\epsilon x^2)^2} \\
        2\epsilon^2 x - \epsilon x^2 = 0 \implies x = 2 \epsilon
\end{align*}
giving that $n > \frac{1}{4\epsilon^2}$ is sufficient for all $x$. Additionally, since $f_n(0) = 0$ for all
$n$, we have that $f_n \to f$ uniformly. Differentiating $f_n$ also shows that $f_n'(x) \to 0$ for all
$x \neq 0$ and that $f_n'(0) = 1 \neq f'(0)$.

\subsection{Exercise 8}
Since $\sum \abs{c_n}$ converges, there exists $N$ such that $n, m \geq N$ gives
\begin{align*}
        \epsilon > \sum_{i = n}^m \abs{c_i} \geq \sum_{i = n}^m \abs{c_i I(x - x_i)} \geq \abs{\sum_{i = n}^m c_i I(x - x_i)}
\end{align*}
so $f(x)$ converges uniformly as $x$ was arbitrary ($0 \leq I(x - x_i) \leq 1$ for all $x$). If $x \neq x_n$ 
and $x$ is not a limit point of $x_n$, then $f(x)$ is clearly continuous since there is a neighborhood
$B_{\delta}(x)$ that contains none of the $x_n$ which implies that $f(x)$ is constant in this neighborhood.
If $x$ is a limit point of the sequence $x_n$, then for any $\epsilon > 0$ we can choose $N$ such that
$n \geq N \implies \abs{c_n} < \epsilon$, so $f $ is continuous at $x$. 

\subsection{Exercise 9}
Since $f_n$ is a sequence of continuous functions that converges uniformly to $f$, $f$ is continuous. By
uniform convergence of $f_n$, we have that there exists 
$N_1 \: | \: n  \geq N_1 \implies \abs{f_n(x) - f(x)} < \frac{\epsilon}{2}$. By continuity of $f$, we have that there
exists $N_2 \: | \: n \geq N_2 \implies \abs{f(x_n) - f(x)} < \frac{\epsilon}{2}$. 
Choosing $n \geq \max(N_1, N_2)$ then gives
\begin{align*}
        \abs{f_n(x_n) - f(x)} &= \abs{f_n(x_n) - f(x_n) + f(x_n) - f(x)} \\
                              &\leq \abs{f_n(x_n) - f(x_n)} + \abs{f(x_n) - f(x)} \\
                              &< \epsilon
\end{align*}
so $\lim_{n \to \infty} f_n(x_n) = f(x)$. The converse need not be true. Consider
\begin{align*}
        f_n(x) = 
        \begin{cases}
                \frac{1}{n} & x = \frac{1}{n} \\
                0 & x \neq \frac{1}{n}
        \end{cases} \\
\end{align*}
which satisfies $\lim_{n \to \infty} f_n(x_n) = 0$ for all sequences $x_n$ despite none of the $f_n$ being
continuous. 

\subsection{Exercise 10}
$f(x)$ converges uniformly since $0 \leq (nx) < 1$. Since $(nx)$ is discontinuous only when $nx$ is an integer,
$f(x)$ is discontinuous only at rational values of $x$ (because we can choose $n = b$ so that 
$n\frac{a}{b} = a$). The continuity of $f(x)$ at irrational values of $x$ comes from the fact that the partial
sums are continuous at all such $x$ and converge uniformly to $f(x)$. Since 
$f_n(x) = \sum_{i = 1}^n \frac{(ix)}{i^2}$ contains only finitely many discontinuities in any bounded interval
(there are only finitely many rational $x = \frac{z}{i}$ such that $z \in [a, b]$), we have that 
$f_n \in \mathscr{R}$. Thus, applying Theorem 7.16, we get that $f \in \mathscr{R}$.
